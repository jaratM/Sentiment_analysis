{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sentiment_analysis_platform.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RHxhF8HncBWF",
        "d1FBKqFz8S6q",
        "TpWLeirYRPZa",
        "yakURjrW8Cqi",
        "vgPy23jc7wSG",
        "g9oE_1oexIW6",
        "EhvEUGhdMD0L",
        "DkOvZjXjlaEq",
        "w_3K-94eLnfS",
        "jwNXUs2JOIHv",
        "7dI3_89PQ9G_",
        "RZKgRlgkRCNI",
        "UAvXKJ12RxEu",
        "BNzITxyJ21l3",
        "rNAiyvNhalTS",
        "VMFGbpw4ldCI",
        "7RYxbNsiNQKk",
        "1cmgJe2LXT2E"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHxhF8HncBWF"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRWrgNjwro2R"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_column', None)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#pre-processing libraries\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from string import punctuation\n",
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive/')\n",
        "from utils import process_tweet, build_freqs, abbreviation\n",
        "\n",
        "!gdown --id 1jdI2jl2W-bFGWMfb59atn28o34GOF3ML\n",
        "# import tweets.csv file \n",
        "!gdown --id 1NDH7awh6eYzxury-qkxAi0UIq6whOUih\n",
        " \n",
        "# ELMo embeddings libraries\n",
        "# !pip install tensorflow==1.15\n",
        "# !pip install tensorflow_hub==0.6.0\n",
        "# import tensorflow_hub as hub\n",
        "# import tensorflow as tf\n",
        "# elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
        "\n",
        "import pickle \n",
        "import pathlib\n",
        "# bert embeddings\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "# gensim libraries \n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "length_w2v = 100\n",
        "\n",
        "# TFIDF \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "# # here we unzip the Glove dataset, with shape 300\n",
        "# !unzip /content/drive/MyDrive/glove.6B.zip\n",
        "\n",
        "# modeling libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# from collections import Counter   \n",
        "# sequence_length = 40\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding,GRU, Dropout, MaxPooling1D\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# warnings.filterwarnings(action='once')\n",
        "\n",
        "\n",
        "Y_train = Y_test = X_train = X_test = None\n",
        "embeddings = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYrqclh9V_BS"
      },
      "source": [
        "def load_data():\n",
        "  # all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "  # all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "  # test_pos = all_positive_tweets[4000:]\n",
        "  # train_pos = all_positive_tweets[:4000]\n",
        "  # test_neg = all_negative_tweets[4000:]\n",
        "  # train_neg = all_negative_tweets[:4000]\n",
        "  # X_train = train_pos + train_neg \n",
        "  # X_test = test_pos + test_neg\n",
        "  # Y_train = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "  # Y_test = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
        "  global Y_train, Y_test, X_train, X_test\n",
        "  df = pd.read_csv(\"tweets.csv\", encoding = \"ISO-8859-1\")\n",
        "  df.columns = [\"target\", \"id\", \"date\", \"flag\", 'user', \"tweet\"]\n",
        "  df = df.drop([ \"id\", \"date\", \"flag\", 'user'], axis=1)\n",
        "  df = df.sample(frac=0.1, random_state=40)\n",
        "  train_data, test_data = train_test_split(df, test_size=0.15,\n",
        "                                         random_state=6)\n",
        "  f = lambda x : 1 if x else 0\n",
        "  train_data['target'] = train_data['target'].map(f)\n",
        "  test_data['target'] = test_data['target'].map(f)\n",
        "  X_train = train_data['tweet'].values.reshape(-1,1)\n",
        "  X_test = test_data['tweet'].values.reshape(-1,1)\n",
        "  Y_train = train_data['target'].values.reshape(-1,1)\n",
        "  Y_test = test_data['target'].values.reshape(-1,1)\n",
        "    \n",
        "  # return X_train, X_test, Y_train, Y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOh6mKNQYW0i"
      },
      "source": [
        "def process_tweet(tweet):\r\n",
        "    \"\"\"Process tweet function.\r\n",
        "    Input:\r\n",
        "        tweet: a string containing a tweet\r\n",
        "    Output:\r\n",
        "        tweets_clean: a list of words containing the processed tweet\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    tweet = str(tweet)\r\n",
        "    stemmer = PorterStemmer()\r\n",
        "    stopwords_english = stopwords.words('english')\r\n",
        "    # remove stock market tickers like $GE\r\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\r\n",
        "    # remove old style retweet text \"RT\"\r\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\r\n",
        "    # remove hyperlinks\r\n",
        "    tweet = re.sub(r'@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+', ' ', tweet)\r\n",
        "    # remove hashtags\r\n",
        "    # only removing the hash # sign from the word\r\n",
        "    tweet = re.sub(r'#', '', tweet)\r\n",
        "    # tokenize tweets\r\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\r\n",
        "                               reduce_len=True)\r\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\r\n",
        "    clean_tweets = []\r\n",
        "    for word in tweet_tokens:\r\n",
        "        if (word not in stopwords_english and  # remove stopwords\r\n",
        "                word not in string.punctuation):  # remove punctuation\r\n",
        "            # clean_tweets.append(word)\r\n",
        "            stem_word = stemmer.stem(word)  # stemming word\r\n",
        "            clean_tweets.append(stem_word)\r\n",
        "    if len(clean_tweets)==0 :\r\n",
        "      clean_tweets.append('UNK')\r\n",
        "    return clean_tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4nXMCHzqrLS"
      },
      "source": [
        "### load embeddings if available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZPv-MY7qppo"
      },
      "source": [
        "\r\n",
        "# this code can be used when words embeddings are saved \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "embeddings = pathlib.Path('/content/drive/MyDrive/embeddings.pickle')\r\n",
        "embeddings_matrices = pathlib.Path('/content/drive/MyDrive/embeddings_matrices.pickle')\r\n",
        "\r\n",
        "  # if files exists we load them instead of training from scratch \r\n",
        "if embeddings.is_file():\r\n",
        "  # w2v, bert, tfidf and glove embeddings \r\n",
        "  pickle_in = open(embeddings, \"rb\")\r\n",
        "  embeddings = pickle.load(pickle_in)\r\n",
        "\r\n",
        "if embeddings_matrices.is_file():\r\n",
        "  # w2v, bert, and glove embeddings_matrices \r\n",
        "  pickle_in = open(embeddings_matrices, \"rb\")\r\n",
        "  embeddings_matrices = pickle.load(pickle_in)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1FBKqFz8S6q"
      },
      "source": [
        "#pre_processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GvpZA9bCkCw"
      },
      "source": [
        "def get_processed_data(tokenized = True):\n",
        "  load_data()\n",
        "  global X_train, X_test\n",
        "  # if(X_train != None and X_test != None):\n",
        "  X_train = [process_tweet(X_train[i]) if tokenized  else  ' '.join(process_tweet(X_train[i])) for i in  range(len(X_train))]\n",
        "  X_test = [process_tweet(X_test[i]) if tokenized  else  ' '.join(process_tweet(X_test[i])) for i in  range(len(X_test))]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpWLeirYRPZa"
      },
      "source": [
        "## Bert "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grOd-5FzROoN"
      },
      "source": [
        "def get_bert():\r\n",
        "  bert_model = SentenceTransformer('bert-base-nli-mean-tokens')\r\n",
        "  # X_train, X_test = get_processed_data()\r\n",
        "  sentences = [\"[CLS] \" + str(TWEET) + \" [SEP]\" for TWEET in X_train]\r\n",
        "  X_train_bert = bert_model.encode(sentences)\r\n",
        "  sentences = [\"[CLS] \" + str(TWEET) + \" [SEP]\" for TWEET in X_test]\r\n",
        "  X_test_bert = bert_model.encode(sentences)\r\n",
        "  return X_train_bert, X_test_bert\r\n",
        "\r\n",
        "\r\n",
        "def get_bert_Seq(vocab):\r\n",
        "  embedding_matrix = np.zeros((len(vocab)+1, 768))\r\n",
        "  bert_model = SentenceTransformer('bert-base-nli-mean-tokens')\r\n",
        "  words = list(vocab.keys())\r\n",
        "  embedding_matrix[1:, :] = np.array(bert_model.encode(words))\r\n",
        "  return embedding_matrix\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yakURjrW8Cqi"
      },
      "source": [
        "##Word2Vec "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBOmmGadzAFg"
      },
      "source": [
        "def get_w2v():\n",
        "  # get_processed_data()\n",
        "  data = np.append(X_train, X_test)\n",
        "  word2vec = Word2Vec(data, size=length_w2v, window=8, min_count=4)\n",
        "  zeros = np.zeros(length_w2v)\n",
        "  X_train_w2v = [ np.sum([ word2vec.wv[x] if x in word2vec.wv.vocab else zeros for x in X_train[i]  ], axis=0)/len(X_train[i]) for i in range(len(X_train))]\n",
        "  X_test_w2v = [ np.sum([ word2vec.wv[x] if x in word2vec.wv.vocab else zeros for x in X_test[i]   ], axis=0)/len(X_test[i])  for i in range(len(X_test))]\n",
        "  return np.array(X_train_w2v), np.array(X_test_w2v), word2vec\n",
        "\n",
        "\n",
        "def get_w2v_seq(vocab):\n",
        "  word2vec = Word2Vec(X_train, size=length_w2v, window=5, min_count=4)\n",
        "  embedding_matrix = np.zeros((len(vocab)+1, length_w2v))\n",
        "  for word, token in vocab.items():\n",
        "      if word2vec.wv.__contains__(word):\n",
        "          embedding_matrix[token] = word2vec.wv.__getitem__(word)\n",
        "  return embedding_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgPy23jc7wSG"
      },
      "source": [
        "## Glove vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToJHCLYhTPf7"
      },
      "source": [
        "def get_glove():\n",
        "  # get_processed_data()\n",
        "  Glove_dict= {}\n",
        "  with open('glove.6B.300d.txt', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "          values = line.split()\n",
        "          word = values[0]\n",
        "          vector = np.asarray(values[1:], \"float32\")\n",
        "          Glove_dict[word] = vector\n",
        "    zeros = np.zeros(300)\n",
        "    X_train_glove = [ np.sum([ Glove_dict.get(x, zeros) for x in X_train[i] ], axis=0)/len(X_train[i])  for i in range(len(X_train)) ]\n",
        "    X_test_glove =  [ np.sum([ Glove_dict.get(x, zeros) for x in X_test[i] ], axis=0)/len(X_test[i])   for i in range(len(X_test)) ]\n",
        "    return np.array(X_train_glove), np.array(X_test_glove), Glove_dict\n",
        "\n",
        "\n",
        "def get_glove_seq(vocab):\n",
        "  embedding_matrix = np.zeros((len(vocab)+1, 300))\n",
        "  Glove_dict= {}\n",
        "  with open('glove.6B.300d.txt', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "          values = line.split()\n",
        "          word = values[0]\n",
        "          vector = np.asarray(values[1:], \"float32\")\n",
        "          Glove_dict[word] = vector\n",
        "  zeros = np.zeros(300)\n",
        "  for word, token in vocab.items():\n",
        "    embedding_matrix[token] = Glove_dict.get(word, zeros)\n",
        "  return embedding_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9oE_1oexIW6"
      },
      "source": [
        "## TF-IDF Vectorizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gMnagqMVBz6"
      },
      "source": [
        "def get_tfidf():\n",
        "  # tweets should be untokenized\n",
        "  get_processed_data(tokenized = False)\n",
        "   \n",
        "  # initialize vectorizer \n",
        "  tfIdfVectorizer=TfidfVectorizer(lowercase=False)\n",
        "\n",
        "  # we fit the training data and we transform the test data \n",
        "  X_train_tfidf = tfIdfVectorizer.fit_transform(X_train)\n",
        "  X_test_tfidf = tfIdfVectorizer.transform(X_test)\n",
        "\n",
        "  return X_train_tfidf, X_test_tfidf, TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhvEUGhdMD0L"
      },
      "source": [
        "## One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YX2p-0kMCX4"
      },
      "source": [
        "def seq_data():\n",
        "  get_processed_data()\n",
        "  tokenizer = Tokenizer(oov_token=\"<oov>\", document_count=0)\n",
        "  tokenizer.fit_on_texts(X_train,)\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "  X_train_pad = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=60)\n",
        "\n",
        "  X_test_pad = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=60)\n",
        "\n",
        "  return X_train_pad, X_test_pad, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAf1tlnjQXJ-"
      },
      "source": [
        "# this code to save the trained tokenizer and embeddings\n",
        "\n",
        "with open('embeddings_matrices.pickle', 'wb') as embed:\n",
        "    pickle.dump(embeddings_matrices, embed, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkOvZjXjlaEq"
      },
      "source": [
        "## ELMo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IySkAKdR9i_m"
      },
      "source": [
        "def elmo_embed(x):\n",
        "  embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    # return average of ELMo features\n",
        "    return sess.run(tf.reduce_mean(embeddings,1))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI6F9KdsHWEz"
      },
      "source": [
        "def get_elmo():\n",
        "\n",
        "  # tr_path = pathlib.Path('/content/drive/MyDrive/X_train_elmo.pickle')\n",
        "  # te_path = pathlib.Path('/content/drive/MyDrive/X_test_elmo.pickle')\n",
        "  \n",
        "  # # if files exists we load them instead of training from scratch \n",
        "  # if tr_path.is_file() and te_path.is_file():\n",
        "  #   pickle_in = open(tr_path, \"rb\")\n",
        "  #   X_elmo_train = pickle.load(pickle_in)\n",
        "  #   pickle_in = open(te_path, \"rb\")\n",
        "  #   X_elmo_test = pickle.load(pickle_in)\n",
        "  #   return X_elmo_train, X_elmo_test\n",
        "\n",
        "  # get processed and untokenized data,\n",
        "  X_train, X_test = get_processed_data(False)\n",
        "\n",
        "  # to speed up transforming words into elmo vectors it's recommanded to transform batches instead of one at a go \n",
        "  X_elmo_train = [X_train[i:i+100] for i in range(0,X_train.shape[0],100)]\n",
        "  X_elmo_test = [X_test[i:i+100] for i in range(0,X_test.shape[0],100)]\n",
        "\n",
        "  # here we start getting the batches embeddings \n",
        "  X_elmo_train = [elmo_embed(x) for x in X_elmo_train]\n",
        "  X_elmo_test = [elmo_embed(x) for x in X_elmo_test]\n",
        "\n",
        "  # now we concatenate back the vectors \n",
        "  X_elmo_train = np.concatenate(X_elmo_train, axis = 0)\n",
        "  X_elmo_test = np.concatenate(X_elmo_test, axis = 0)\n",
        "  # we save the vectors as pickle files\n",
        "  pickle_out = open(\"X_train_elmo.pickle\",\"wb\")\n",
        "  pickle.dump(X_elmo_train, pickle_out)\n",
        "  pickle_out.close()\n",
        "\n",
        "  pickle_out = open(\"X_test_elmo.pickle\",\"wb\")\n",
        "  pickle.dump(X_elmo_test, pickle_out)\n",
        "  pickle_out.close()\n",
        "\n",
        "  return X_elmo_train, X_elmo_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_3K-94eLnfS"
      },
      "source": [
        "## load data and embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI2WJHd8t048"
      },
      "source": [
        "def get_embeddings():\r\n",
        "  get_processed_data()\r\n",
        "  embeddings = {\r\n",
        "                'glove': (get_glove()),\r\n",
        "                'w2v': (get_w2v()),\r\n",
        "                'bert':(get_bert())\r\n",
        "                }\r\n",
        "  get_processed_data(False)\r\n",
        "  embeddings['tfidf'] = get_tfidf()\r\n",
        "  return embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhaozHIPC50B"
      },
      "source": [
        "# embeddings = get_embeddings()\n",
        "# with open('embeddings.pickle', 'wb') as embed:\n",
        "#     pickle.dump(embeddings, embed, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIJzVZqN6c47"
      },
      "source": [
        "def get_seq_embeddings(vocab):\n",
        "  embeddings_matrices = {\n",
        "                'glove': (get_glove_seq(vocab)),\n",
        "                'w2v': (get_w2v_seq(vocab)),\n",
        "                'bert':(get_bert_Seq(vocab))\n",
        "                }\n",
        "  return embeddings_matrices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvIun95kRs4x"
      },
      "source": [
        "# Models and architectures \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwNXUs2JOIHv"
      },
      "source": [
        "##RNN based models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dI3_89PQ9G_"
      },
      "source": [
        "### architectures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeCoqoha4cC9"
      },
      "source": [
        "X_train_pad, X_test_pad, tokenizer = seq_data()\n",
        "# embeddings_matrices = get_seq_embeddings(tokenizer.word_index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xgbuzRDdYqf"
      },
      "source": [
        "def lstm(embedding_matrix):\n",
        "  embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[embedding_matrix], input_length=60)\n",
        "  #  LSTM model with 100   \n",
        "  model = Sequential()\n",
        "  model.add(embedding_layer)\n",
        "  model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model_history=model.fit(X_train_pad, Y_train,batch_size=1024,epochs=3, validation_split=0.1, verbose=1)\n",
        "  scores = model.predict(X_test_pad)\n",
        "  y_pred=np.where(scores>0.5,1,0)\n",
        "  accu = accuracy_score(Y_test, y_pred)\n",
        "  print(accu)\n",
        "  return accu, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38KSkQOQxIOk"
      },
      "source": [
        "def Bidirectional_lstm(embedding_matrix):\n",
        "  input_shape = embedding_matrix.shape[0], embedding_matrix.shape[1]\n",
        "  embedding_layer = Embedding(input_shape[0], input_shape[1], weights=[embedding_matrix], input_length=60)\n",
        "  #  LSTM model with 100   \n",
        "  # Two stacked Bidirectional LSTM\n",
        "  model = Sequential([\n",
        "        embedding_layer,\n",
        "        Bidirectional(LSTM(embedding_matrix.shape[1], return_sequences=True), input_shape=input_shape),\n",
        "        Bidirectional(LSTM(60)),\n",
        "        Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model_history=model.fit(X_train_pad, Y_train,batch_size=1024,epochs=3, validation_split=0.1, verbose=1)\n",
        "  scores = model.predict(X_test_pad)\n",
        "  y_pred=np.where(scores>0.5,1,0)\n",
        "  accu = accuracy_score(Y_test, y_pred)\n",
        "  print(accu)\n",
        "  return accu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzfLccH4y7-r"
      },
      "source": [
        "def mixed_lstm( embedding_matrix):\n",
        "  embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[embedding_matrix], input_length=60)\n",
        "  #  LSTM model with 100   \n",
        "  model = Sequential()\n",
        "  model.add(embedding_layer)\n",
        "  model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model_history=model.fit(X_train_pad, Y_train,batch_size=1024,epochs=3, validation_split=0.1, verbose=1)\n",
        "  scores = model.predict(X_test_pad)\n",
        "  y_pred=np.where(scores>0.5,1,0)\n",
        "  accu = accuracy_score(Y_test, y_pred)\n",
        "  print(accu)\n",
        "  return accu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DFL6f8OveOu"
      },
      "source": [
        "def dropout_lstm(embedding_matrix):\n",
        "    embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[embedding_matrix], input_length=60)\n",
        "    #  LSTM model with 100   \n",
        "    model = Sequential()\n",
        "    model.add(embedding_layer)\n",
        "    model.add(Bidirectional(LSTM(60)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model_history=model.fit(X_train_pad, Y_train,batch_size=1024,epochs=3, validation_split=0.1, verbose=1)\n",
        "    scores = model.predict(X_test_pad)\n",
        "    y_pred=np.where(scores>0.5,1,0)\n",
        "    accu = accuracy_score(Y_test, y_pred)\n",
        "    print(accu)\n",
        "    return accu\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbmkuFAGjfin"
      },
      "source": [
        "def Gru(embedding_matrix):\n",
        "  embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[embedding_matrix], input_length=60)\n",
        "  model = Sequential([\n",
        "        embedding_layer,\n",
        "        GRU(60),\n",
        "        Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer=\"adam\",\n",
        "              metrics=['accuracy'])\n",
        "  model_history=model.fit(X_train_pad, Y_train,batch_size=1024,epochs=3, validation_split=0.1, verbose=1)\n",
        "  scores = model.predict(X_test_pad)\n",
        "  y_pred=np.where(scores>0.5,1,0)\n",
        "  accu = accuracy_score(Y_test, y_pred)\n",
        "  print(accu)\n",
        "  return accu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZKgRlgkRCNI"
      },
      "source": [
        "### predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtnNaFs-GKrl"
      },
      "source": [
        "# Global RNN based models scores\n",
        "R_scores = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoTgt_oSk9nF",
        "outputId": "00777568-2960-487e-ab2d-18ec7cb22040"
      },
      "source": [
        "\n",
        "def lstm_scores(embedding_matrix):\n",
        "  lstm_score = {}\n",
        "  for name, embed in embedding_matrix.items():\n",
        "    if (name == 'bert'):\n",
        "      lstm_score[name], model = lstm(embed)\n",
        "  return lstm_score, model\n",
        "lstm_score, model = lstm_scores(embeddings_matrices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/3\n",
            "120/120 [==============================] - 127s 1s/step - loss: 0.6448 - accuracy: 0.6257 - val_loss: 0.5047 - val_accuracy: 0.7500\n",
            "Epoch 2/3\n",
            "120/120 [==============================] - 122s 1s/step - loss: 0.4831 - accuracy: 0.7662 - val_loss: 0.4708 - val_accuracy: 0.7700\n",
            "Epoch 3/3\n",
            "120/120 [==============================] - 122s 1s/step - loss: 0.4175 - accuracy: 0.8115 - val_loss: 0.4878 - val_accuracy: 0.7664\n",
            "0.7607083333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wa0JkOhcavL",
        "outputId": "607a61e0-8075-40b9-e2f2-22301bab311a"
      },
      "source": [
        "lstm_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bert': 0.769, 'glove': 0.7574583333333333, 'w2v': 0.7624166666666666}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6fKeMuJGUM_"
      },
      "source": [
        "R_scores['lstm'] = lstm_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQSRkhgBzdnl",
        "outputId": "c46a1dc9-32eb-46a0-c090-7baa9e1f3109"
      },
      "source": [
        "def mixed_lstm_scores(embedding_matrix):\n",
        "  lstm_score = {}\n",
        "  for name, embed in embedding_matrix.items():\n",
        "    lstm_score[name] = mixed_lstm(embed)\n",
        "  return lstm_score\n",
        "mixed_lstm_score = mixed_lstm_scores(embeddings_matrices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "120/120 [==============================] - 45s 301ms/step - loss: 0.6071 - accuracy: 0.6588 - val_loss: 0.4929 - val_accuracy: 0.7568\n",
            "Epoch 2/3\n",
            "120/120 [==============================] - 35s 291ms/step - loss: 0.4464 - accuracy: 0.7958 - val_loss: 0.4801 - val_accuracy: 0.7668\n",
            "Epoch 3/3\n",
            "120/120 [==============================] - 35s 293ms/step - loss: 0.3438 - accuracy: 0.8547 - val_loss: 0.5227 - val_accuracy: 0.7565\n",
            "0.752625\n",
            "Epoch 1/3\n",
            "120/120 [==============================] - 15s 114ms/step - loss: 0.5926 - accuracy: 0.6855 - val_loss: 0.5022 - val_accuracy: 0.7507\n",
            "Epoch 2/3\n",
            "120/120 [==============================] - 13s 109ms/step - loss: 0.4758 - accuracy: 0.7739 - val_loss: 0.4754 - val_accuracy: 0.7724\n",
            "Epoch 3/3\n",
            "120/120 [==============================] - 13s 107ms/step - loss: 0.3913 - accuracy: 0.8272 - val_loss: 0.5119 - val_accuracy: 0.7613\n",
            "0.7527083333333333\n",
            "Epoch 1/3\n",
            "120/120 [==============================] - 98s 796ms/step - loss: 0.6067 - accuracy: 0.6575 - val_loss: 0.4926 - val_accuracy: 0.7594\n",
            "Epoch 2/3\n",
            "120/120 [==============================] - 93s 774ms/step - loss: 0.4642 - accuracy: 0.7805 - val_loss: 0.4816 - val_accuracy: 0.7640\n",
            "Epoch 3/3\n",
            "120/120 [==============================] - 93s 777ms/step - loss: 0.3911 - accuracy: 0.8263 - val_loss: 0.4769 - val_accuracy: 0.7735\n",
            "0.765375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGup_p7Al085",
        "outputId": "0ce37847-60fb-4743-956d-6866fcb2e12f"
      },
      "source": [
        "mixed_lstm_score "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bert': 0.765375, 'glove': 0.752625, 'w2v': 0.7527083333333333}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuSvqAFkGabK"
      },
      "source": [
        "R_scores['mixed_lstm'] = mixed_lstm_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC1p9vH5LEaV",
        "outputId": "b3fe867a-3159-4ea3-8e6d-fbaf344e343a"
      },
      "source": [
        "def dropout_lstm_scores(embedding_matrix):\n",
        "  lstm_score = {}\n",
        "  for name, embed in embedding_matrix.items():\n",
        "    lstm_score[name] = dropout_lstm(embed)\n",
        "  return lstm_score\n",
        "dropout_lstm_score = dropout_lstm_scores(embeddings_matrices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "120/120 [==============================] - 61s 432ms/step - loss: 0.6073 - accuracy: 0.6618 - val_loss: 0.4977 - val_accuracy: 0.7566\n",
            "Epoch 2/3\n",
            "120/120 [==============================] - 50s 418ms/step - loss: 0.4626 - accuracy: 0.7882 - val_loss: 0.4795 - val_accuracy: 0.7697\n",
            "Epoch 3/3\n",
            "120/120 [==============================] - 49s 411ms/step - loss: 0.3837 - accuracy: 0.8345 - val_loss: 0.5152 - val_accuracy: 0.7550\n",
            "0.7499583333333333\n",
            "Epoch 1/3\n",
            "120/120 [==============================] - 25s 179ms/step - loss: 0.6048 - accuracy: 0.6635 - val_loss: 0.5045 - val_accuracy: 0.7547\n",
            "Epoch 2/3\n",
            "120/120 [==============================] - 22s 181ms/step - loss: 0.4812 - accuracy: 0.7715 - val_loss: 0.4751 - val_accuracy: 0.7696\n",
            "Epoch 3/3\n",
            "120/120 [==============================] - 21s 172ms/step - loss: 0.4108 - accuracy: 0.8175 - val_loss: 0.4919 - val_accuracy: 0.7651\n",
            "0.7582916666666667\n",
            "Epoch 1/3\n",
            "120/120 [==============================] - 114s 916ms/step - loss: 0.6265 - accuracy: 0.6420 - val_loss: 0.5233 - val_accuracy: 0.7431\n",
            "Epoch 2/3\n",
            "120/120 [==============================] - 108s 901ms/step - loss: 0.4891 - accuracy: 0.7678 - val_loss: 0.4754 - val_accuracy: 0.7723\n",
            "Epoch 3/3\n",
            "120/120 [==============================] - 108s 898ms/step - loss: 0.3992 - accuracy: 0.8245 - val_loss: 0.5168 - val_accuracy: 0.7516\n",
            "0.7545833333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvrnCibWv1iE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811ee0c8-1226-4368-c6e7-87df056ed486"
      },
      "source": [
        "dropout_lstm_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bert': 0.7545833333333334,\n",
              " 'glove': 0.7499583333333333,\n",
              " 'w2v': 0.7582916666666667}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3cU5t6MGdzg"
      },
      "source": [
        "R_scores['dropout_lstm'] = dropout_lstm_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O11k94I_9SFn",
        "outputId": "88e4db53-8ba5-4b2f-955d-52a9ebf92dda"
      },
      "source": [
        "def Bidirectional_lstm_scores(embedding_matrix):\n",
        "  bidirectional_score = {}\n",
        "  for name, embed in embedding_matrix.items():\n",
        "    bidirectional_score[name] = Bidirectional_lstm(embed)\n",
        "  return bidirectional_score\n",
        "bidirectional_score = Bidirectional_lstm_scores(embeddings_matrices)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "120/120 [==============================] - 106s 828ms/step - loss: 0.5761 - accuracy: 0.6889 - val_loss: 0.4849 - val_accuracy: 0.7625\n",
            "Epoch 2/3\n",
            "120/120 [==============================] - 98s 816ms/step - loss: 0.4391 - accuracy: 0.7970 - val_loss: 0.4797 - val_accuracy: 0.7688\n",
            "Epoch 3/3\n",
            "120/120 [==============================] - 97s 813ms/step - loss: 0.3382 - accuracy: 0.8549 - val_loss: 0.5426 - val_accuracy: 0.7525\n",
            "0.7517916666666666\n",
            "Epoch 1/3\n",
            "120/120 [==============================] - 42s 291ms/step - loss: 0.5733 - accuracy: 0.6983 - val_loss: 0.5014 - val_accuracy: 0.7555\n",
            "Epoch 2/3\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.4570 - accuracy: 0.7874 - val_loss: 0.4826 - val_accuracy: 0.7656\n",
            "Epoch 3/3\n",
            "120/120 [==============================] - 34s 281ms/step - loss: 0.3542 - accuracy: 0.8479 - val_loss: 0.5091 - val_accuracy: 0.7603\n",
            "0.7545\n",
            "Epoch 1/3\n",
            "120/120 [==============================] - 339s 3s/step - loss: 0.6953 - accuracy: 0.5785 - val_loss: 0.5313 - val_accuracy: 0.7395\n",
            "Epoch 2/3\n",
            "120/120 [==============================] - 333s 3s/step - loss: 0.5091 - accuracy: 0.7532 - val_loss: 0.4808 - val_accuracy: 0.7684\n",
            "Epoch 3/3\n",
            "120/120 [==============================] - 330s 3s/step - loss: 0.4463 - accuracy: 0.7928 - val_loss: 0.4814 - val_accuracy: 0.7629\n",
            "0.7671666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl1da67uFXtB",
        "outputId": "1a64ca44-b4b9-4bfc-aeef-d1c86567f858"
      },
      "source": [
        "bidirectional_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bert': 0.7671666666666667, 'glove': 0.7517916666666666, 'w2v': 0.7545}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoBz7xJ-GpTK"
      },
      "source": [
        "R_scores['bidirectional_lstm'] = bidirectional_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd7qf-GppHqe",
        "outputId": "4e068b70-ac0b-4776-c3bc-6e6c76c62c67"
      },
      "source": [
        "def gru_scores(embedding_matrix):\n",
        "  gru_score = {}\n",
        "  for name, embed in embedding_matrix.items():\n",
        "    gru_score[name] = Gru(embed)\n",
        "  return gru_score\n",
        "gru_score = gru_scores(embeddings_matrices)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "120/120 [==============================] - 49s 388ms/step - loss: 0.5966 - accuracy: 0.6676 - val_loss: 0.4893 - val_accuracy: 0.7621\n",
            "Epoch 2/3\n",
            "120/120 [==============================] - 47s 388ms/step - loss: 0.4385 - accuracy: 0.7998 - val_loss: 0.4857 - val_accuracy: 0.7671\n",
            "Epoch 3/3\n",
            "120/120 [==============================] - 46s 384ms/step - loss: 0.3424 - accuracy: 0.8552 - val_loss: 0.5407 - val_accuracy: 0.7489\n",
            "0.74325\n",
            "Epoch 1/3\n",
            "120/120 [==============================] - 20s 154ms/step - loss: 0.6050 - accuracy: 0.6568 - val_loss: 0.4986 - val_accuracy: 0.7584\n",
            "Epoch 2/3\n",
            "120/120 [==============================] - 17s 145ms/step - loss: 0.4688 - accuracy: 0.7784 - val_loss: 0.4725 - val_accuracy: 0.7751\n",
            "Epoch 3/3\n",
            "120/120 [==============================] - 18s 146ms/step - loss: 0.3579 - accuracy: 0.8481 - val_loss: 0.5379 - val_accuracy: 0.7524\n",
            "0.7454583333333333\n",
            "Epoch 1/3\n",
            "120/120 [==============================] - 102s 829ms/step - loss: 0.6224 - accuracy: 0.6412 - val_loss: 0.4901 - val_accuracy: 0.7594\n",
            "Epoch 2/3\n",
            "120/120 [==============================] - 99s 826ms/step - loss: 0.4655 - accuracy: 0.7784 - val_loss: 0.4709 - val_accuracy: 0.7704\n",
            "Epoch 3/3\n",
            "120/120 [==============================] - 99s 827ms/step - loss: 0.3828 - accuracy: 0.8325 - val_loss: 0.5020 - val_accuracy: 0.7610\n",
            "0.7592083333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5skDt0b7vmY",
        "outputId": "b8a9af25-ec2e-4de5-f158-83061f721600"
      },
      "source": [
        "gru_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bert': 0.7592083333333334, 'glove': 0.74325, 'w2v': 0.7454583333333333}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SWtW7T0GyIS"
      },
      "source": [
        "R_scores['gru'] = gru_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxmHaXALG35Y"
      },
      "source": [
        "for key in R_scores.keys():\n",
        "  R_scores[key]['tfidf'] = '--'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAvXKJ12RxEu"
      },
      "source": [
        "## Classic algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNzITxyJ21l3"
      },
      "source": [
        "### algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gR7ji2rRrcu"
      },
      "source": [
        "def logistic_model(X_tr, y_tr, X_test, y_test, description):\r\n",
        "    clf = LogisticRegression(max_iter=1000).fit(X_tr, y_tr)\r\n",
        "    score = clf.score(X_test, y_test)\r\n",
        "    return score, clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DshVCdD2I3G"
      },
      "source": [
        "def svm_model(X_train, Y_train, X_test, Y_test, description):\r\n",
        "   clf = LinearSVC(random_state=0, tol=1e-5)\r\n",
        "   clf.fit(X_train, Y_train)\r\n",
        "   score = clf.score(X_test, Y_test)\r\n",
        "   return score, clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2tQIn-pTA9X"
      },
      "source": [
        "def AdaBoost_model(X_tr, y_tr, X_test, y_test, description):\r\n",
        "  clf = AdaBoostClassifier(n_estimators=100, random_state=0)\r\n",
        "  clf.fit(X_tr, y_tr)\r\n",
        "  score = clf.score(X_test, y_test)\r\n",
        "  return score, clf\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ls1e8wPWxrQ"
      },
      "source": [
        "def Xgboost_model(X_tr, y_tr, X_test, y_test, description):\r\n",
        "  clf = XGBClassifier(random_state=0)\r\n",
        "  clf.fit(X_tr, y_tr)\r\n",
        "  score = clf.score(X_test, y_test)\r\n",
        "  return score, clf \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEEPFJRofSd1"
      },
      "source": [
        "### predicting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNAiyvNhalTS"
      },
      "source": [
        "#### lg_Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkd47K6Wv0Rv"
      },
      "source": [
        "def get_lg_scores(embeddings):\r\n",
        "  scores = []\r\n",
        "  lg_models = []\r\n",
        "  for key,value in embeddings.items():\r\n",
        "    score, model = logistic_model(value[0], Y_train, value[1], Y_test, key)\r\n",
        "    scores.append(score)\r\n",
        "    lg_models.append(model)\r\n",
        "  return scores, lg_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU3FofTqzZAK"
      },
      "source": [
        "lg_score, lg_models = get_lg_scores(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG4VCQXvk6wK"
      },
      "source": [
        "scores = { \"logistic_regression\": lg_score}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMFGbpw4ldCI"
      },
      "source": [
        "#### SVM_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHmGs-yBlgSb"
      },
      "source": [
        "def get_svm_scores(embeddings):\r\n",
        "  scores = []\r\n",
        "  svm_models = []\r\n",
        "  for key,value in embeddings.items():\r\n",
        "    score, model = svm_model(value[0], Y_train, value[1], Y_test, key)\r\n",
        "    scores.append(score)\r\n",
        "    # svm_models.append[model]\r\n",
        "  return scores,svm_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu782Dghsnku"
      },
      "source": [
        "svm_scores, svm_models = get_svm_scores(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufB-Wjeo_3B-"
      },
      "source": [
        "scores[\"svm\"] = svm_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RYxbNsiNQKk"
      },
      "source": [
        "#### AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLSP-fKRNTmA"
      },
      "source": [
        "def get_AdaBoost_scores(embeddings):\r\n",
        "  scores = []\r\n",
        "  ada_models = []\r\n",
        "  for key,value in embeddings.items():\r\n",
        "    score, model = AdaBoost_model(value[0], Y_train, value[1], Y_test, key)\r\n",
        "    scores.append(score)\r\n",
        "    # ada_models.append(model)\r\n",
        "  return scores, ada_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GetOPbGeNa-1"
      },
      "source": [
        "\r\n",
        "AdaBoost_scores, ada_models = get_AdaBoost_scores(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5MD3JxoO41W"
      },
      "source": [
        "scores[\"AdaBoost\"] = AdaBoost_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cmgJe2LXT2E"
      },
      "source": [
        "#### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7cOiM2QXWHv"
      },
      "source": [
        "def get_XGBoost_scores(embeddings):\r\n",
        "  scores = []\r\n",
        "  XGB_models = []\r\n",
        "  for key,value in embeddings.items():\r\n",
        "    score, model = Xgboost_model(value[0], Y_train, value[1], Y_test, key)\r\n",
        "    scores.append(score)\r\n",
        "    # XGB_models.append(model)\r\n",
        "  return scores "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGP6dbe3Xckl"
      },
      "source": [
        "XGBoost_scores = get_XGBoost_scores(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Gf9zwiXlpk"
      },
      "source": [
        "scores[\"XGBoost\"] = XGBoost_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fQo4a09BO2D"
      },
      "source": [
        "# results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBatVzndAHvh"
      },
      "source": [
        "results = pd.DataFrame.from_dict(scores, orient='index',columns=[key for key in embeddings.keys()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "IU0yfhC8AVFx",
        "outputId": "3eda2a25-7e24-48a3-ef5a-f6df80c3dfb8"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>glove</th>\n",
              "      <th>w2v</th>\n",
              "      <th>bert</th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>logistic_regression</th>\n",
              "      <td>0.698625</td>\n",
              "      <td>0.728417</td>\n",
              "      <td>0.721500</td>\n",
              "      <td>0.765167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>svm</th>\n",
              "      <td>0.699125</td>\n",
              "      <td>0.728917</td>\n",
              "      <td>0.718125</td>\n",
              "      <td>0.751333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AdaBoost</th>\n",
              "      <td>0.667958</td>\n",
              "      <td>0.715958</td>\n",
              "      <td>0.686125</td>\n",
              "      <td>0.704125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost</th>\n",
              "      <td>0.681333</td>\n",
              "      <td>0.721583</td>\n",
              "      <td>0.693333</td>\n",
              "      <td>0.680167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        glove       w2v      bert     tfidf\n",
              "logistic_regression  0.698625  0.728417  0.721500  0.765167\n",
              "svm                  0.699125  0.728917  0.718125  0.751333\n",
              "AdaBoost             0.667958  0.715958  0.686125  0.704125\n",
              "XGBoost              0.681333  0.721583  0.693333  0.680167"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iuapx8XeS1t"
      },
      "source": [
        "R_results = pd.DataFrame.from_dict(R_scores, orient='index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "C4cXssb-Ho3F",
        "outputId": "052cbef1-2daa-4046-e47a-7db98d485e54"
      },
      "source": [
        "R_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bert</th>\n",
              "      <th>glove</th>\n",
              "      <th>w2v</th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>lstm</th>\n",
              "      <td>0.769000</td>\n",
              "      <td>0.757458</td>\n",
              "      <td>0.762417</td>\n",
              "      <td>--</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mixed_lstm</th>\n",
              "      <td>0.765375</td>\n",
              "      <td>0.752625</td>\n",
              "      <td>0.752708</td>\n",
              "      <td>--</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dropout_lstm</th>\n",
              "      <td>0.754583</td>\n",
              "      <td>0.749958</td>\n",
              "      <td>0.758292</td>\n",
              "      <td>--</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bidirectional_lstm</th>\n",
              "      <td>0.767167</td>\n",
              "      <td>0.751792</td>\n",
              "      <td>0.754500</td>\n",
              "      <td>--</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gru</th>\n",
              "      <td>0.759208</td>\n",
              "      <td>0.743250</td>\n",
              "      <td>0.745458</td>\n",
              "      <td>--</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        bert     glove       w2v tfidf\n",
              "lstm                0.769000  0.757458  0.762417    --\n",
              "mixed_lstm          0.765375  0.752625  0.752708    --\n",
              "dropout_lstm        0.754583  0.749958  0.758292    --\n",
              "bidirectional_lstm  0.767167  0.751792  0.754500    --\n",
              "gru                 0.759208  0.743250  0.745458    --"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    }
  ]
}