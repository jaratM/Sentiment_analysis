{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/mohamed/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mohamed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snscrape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7d6752ebeccd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msnscrape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwitter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msntwitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'snscrape'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# import config\n",
    "# from tweepy import OAuthHandler, Stream\n",
    "# from tweepy.streaming import StreamListener\n",
    "import json\n",
    "# import pymongo\n",
    "import requests\n",
    "from urllib3.exceptions import ProtocolError\n",
    "\n",
    "# pre_processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from string import punctuation\n",
    "nltk.download('twitter_samples')\n",
    "from nltk.corpus import twitter_samples\n",
    "nltk.download('stopwords')\n",
    "import pickle \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# connect to local MongoDB\n",
    "# client = pymongo.MongoClient(\"mongodb://127.0.0.1\", 27017)\n",
    "# db = client.sentiment_analysis\n",
    "# model =  pickle.load(open(\"model.pkl\", \"rb\"))\n",
    "\n",
    "def process_tweet(tweet):\n",
    "\n",
    "    tweet = str(tweet)\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+', ' ', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "    if len(tweets_clean)==0 :\n",
    "      tweets_clean.append('UNK')\n",
    "    return tweets_clean\n",
    "\n",
    "import os\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "\n",
    "tweet_count = 50\n",
    "text_query = \"bitcoin\"\n",
    "until_date = [\"2020-11-26\",\"2020-11-30\", \"2020-11-29\", \"2020-11-28\", \"2020-11-27\" ]\n",
    "city_coord = [\"37.7764685,-122.4172004\", \"40.7128,-74.0060\", \"51.5074,-0.1278\", \"35.6897,139.6922\", \"48.856613,2.352222\"]\n",
    "cities_name = [\"San Francisco\", \"New york\", \"London\", \"tokyo\", \"paris\"]\n",
    "tweets = None\n",
    "\n",
    "for date in until_date:\n",
    "\n",
    "\tfor i in range(len(cities_name)):\n",
    "\n",
    "\t\tos.system('snscrape --jsonl --max-results {} twitter-search \"{} geocode:{},10km until:{}\" > text-query-tweets.json'.\n",
    "\t\tformat(tweet_count, text_query, city_coord[i], date))\n",
    "\t\ttweets_df = pd.read_json('text-query-tweets.json', lines=True)\n",
    "\t\ttweets_df['location'] = cities_name[i]\n",
    "\t\ttweets_df['date'] = date\n",
    "\t\ttweets_df['sentiment'] = 0\n",
    "\t\tif tweets is None:\n",
    "\t\t\ttweets = tweets_df\n",
    "\t\telse :\n",
    "\t\t\ttweets = pd.concat([tweets,tweets_df])\n",
    "\t\n",
    "tweets = tweets[['content', 'sentiment','location', 'date']]\n",
    "\n",
    "# db.tweets.insert_many(tweets.to_dict('records'))\n",
    "\n",
    "import random \n",
    "\n",
    "tweets['sentiment'] = [random.randint(0,1) for _ in range(5*5*50) ]\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "dates = tweets.date.unique()\n",
    "cities = tweets.location.unique()\n",
    "\n",
    "pos_daily = tweets[tweets['sentiment']==1].groupby('date')['sentiment'].value_counts()\n",
    "neg_daily = tweets[tweets['sentiment']==0].groupby('date')['sentiment'].value_counts()\n",
    "\n",
    "pos_locally = tweets[tweets['sentiment']==1].groupby('location')['sentiment'].value_counts()\n",
    "neg_locally = tweets[tweets['sentiment']==0].groupby('location')['sentiment'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "                      \n",
    "                      go.Bar(name='Positive daily sentiment', x = dates, y = [pos_daily['{}'.format(d),1] for d in dates]),\n",
    "                      \n",
    "                      go.Bar(name='Negative daily sentiment', x = dates, y = [neg_daily['{}'.format(d),0] for d in dates])\n",
    "\n",
    "])\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "                      \n",
    "                      go.Bar(name='Positive regional sentiment', x = cities, y = [pos_locally['{}'.format(d),1] for d in cities]),\n",
    "                      \n",
    "                      go.Bar(name='Negative regional sentiment', x = cities, y = [neg_locally['{}'.format(d),0] for d in cities])\n",
    "\n",
    "])\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# os.system(\"snscrape --jsonl --max-results 500 --since 2020-06-01 twitter-search \\\"its the elephant until:2020-07-31\\\" > text-query-tweets.json\")\n",
    "\n",
    "# import json\n",
    "  \n",
    "# Opening JSON file \n",
    "# with open('text-query-tweets.json', 'r') as f:\n",
    "#   data = json.load(f)\n",
    "\n",
    "# content = []\n",
    "# location = []\n",
    "# for doc in data:\n",
    "# \tprint(doc['content'])\n",
    "# \tprint(doc['user']['location'])\n",
    "\n",
    "# tweets_df = pd.DataFrame()\n",
    "# tweets_df['location'] = location\n",
    "# tweets_df['content'] = content\n",
    "\n",
    "# # tweets_df = tweets_df[['content', 'location']]\n",
    "# print('hhhhhhhhhhhhh \\n hhhhhh')\n",
    "# print(len(tweets_df))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def authenticate():\n",
    "#     \"\"\"Function for handling Twitter Authentication. Please note\n",
    "#        that this script assumes you have a file called config.py\n",
    "#        which stores the 4 required authentication tokens:\n",
    "#        1. CONSUMER_API_KEY\n",
    "#        2. CONSUMER_API_SECRET\n",
    "#        3. ACCESS_TOKEN\n",
    "#        4. ACCESS_TOKEN_SECRET\n",
    "#     See course material for instructions on getting your own Twitter credentials.\n",
    "#     \"\"\"\n",
    "#     auth = OAuthHandler(config.CONSUMER_API_KEY, config.CONSUMER_API_SECRET)\n",
    "#     auth.set_access_token(config.ACCESS_TOKEN, config.ACCESS_TOKEN_SECRET)\n",
    "\n",
    "#     return auth\n",
    "\n",
    "# # def getLocationOfCity(city):\n",
    "# #     api_key = config.OPEN_CAGE_API_KEY\n",
    "# #     req = requests.get('https://api.opencagedata.com/geocode/v1/json?q='+city+'&key='+api_key)\n",
    "# #     js = json.loads(req.text)\n",
    "# #     rep = None\n",
    "# #     for res in js['results']:\n",
    "# #         if res['formatted'].split(',')[-1] == \" United States of America\":\n",
    "# #             rep = res['geometry']\n",
    "# #             break\n",
    "# #     return rep\n",
    "# def predict(tweet):\n",
    "\n",
    "#   \ttweet_pad = pad_sequences(tokenizer.texts_to_sequences(tweet), maxlen=60)\n",
    "#   \tscore = model.predict(tweet_pad)\n",
    "#   \tsentiment = np.where(score > 0.5, 1, 0)\n",
    "#   \treturn sentiment\n",
    "\n",
    "# class TwitterListener(StreamListener):\n",
    "# \tdef on_data(self, data):\n",
    "\n",
    "# \t\ttry:\n",
    "# \t\t\tt = json.loads(data)  \n",
    "# \t\t\ttext = process_tweet(t[\"text\"])\n",
    "# \t\t\tsentiment = predict(text)\n",
    "# \t\t\tlocation = None\n",
    "# \t\t\tif t[\"user\"][\"location\"] is not None:\n",
    "# \t\t\t\tlocation = t[\"user\"][\"location\"]\n",
    "\n",
    "# \t\t\tif location is not None:\n",
    "# \t\t\t\ttweet = {\n",
    "# \t\t\t\t\t\"text\": text,\n",
    "# \t\t\t\t\t\"timestamp\": datetime.strptime(t[\"created_at\"], \"%a %b %d %H:%M:%S +0000 %Y\"),\n",
    "# \t\t\t\t\t\"location\" : location\n",
    "# \t\t\t\t}\n",
    "\n",
    "# \t\t\t\t# write to mongo collection 'tweets'\n",
    "# \t\t\t\tdb.tweets.insert_one(tweet)\n",
    "# \t\t\t\t# print(tweet)\n",
    "# \t\texcept Exception as e:\n",
    "# \t\t\tprint(e)\n",
    "\n",
    "# \tdef on_error(self, status):\n",
    "# \t\tif status == 420:\n",
    "# \t\t\tprint(status)\n",
    "# \t\t\treturn False\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     auth = authenticate()\n",
    "#     listener = TwitterListener()\n",
    "#     stream = Stream(auth, listener)\n",
    "#     i = 0\n",
    "#     while True:\n",
    "#     \ttry:\n",
    "#     \t\tstream.filter(\n",
    "# \t\t\t\ttrack=[\"bitcoin\"],\n",
    "# \t\t\t\tlanguages=[\"en\"],\n",
    "# \t\t\t\tstall_warnings=True\n",
    "# \t\t\t)\n",
    "#     \texcept ProtocolError:\n",
    "#     \t\tcontinue\n",
    "#     \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
